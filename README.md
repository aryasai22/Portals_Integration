# CEIPAL ETL Pipeline

Enterprise-grade **Extract, Transform, Load (ETL)** pipeline for extracting workforce data from CEIPAL API and loading into Snowflake Data Warehouse using PySpark for distributed processing and Power BI for visualization.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Usage](#usage)
- [Data Pipeline](#data-pipeline)
- [Power BI Integration](#power-bi-integration)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

---

## ğŸ¯ Overview

This ETL pipeline automates the extraction of workforce management data from CEIPAL and provides a complete analytics solution:

```
CEIPAL API â†’ PySpark ETL â†’ Snowflake Data Warehouse â†’ Power BI Dashboards
```

### **Key Entities Extracted:**
- **Employees** - Personnel information, departments, roles
- **Projects** - Project details, timelines, managers
- **Placements** - Job assignments, billing rates, pay rates
- **Clients** - Client information and contracts
- **Vendors** - Vendor relationships
- **Expenses** - Employee expenses and reimbursements
- **Invoices** - Billing and payment records
- **Countries & States** - Geographic reference data

---

## ğŸ—ï¸ Architecture

### **Data Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PIPELINE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. EXTRACTION (Source)                                     â”‚
â”‚     â”œâ”€ CEIPAL REST API                                      â”‚
â”‚     â”œâ”€ Authentication & token management                    â”‚
â”‚     â”œâ”€ Rate limiting & retry logic                          â”‚
â”‚     â””â”€ Parallel data enrichment                             â”‚
â”‚                                                              â”‚
â”‚                        â†“                                     â”‚
â”‚                                                              â”‚
â”‚  2. TRANSFORMATION (Processing)                             â”‚
â”‚     â”œâ”€ PySpark distributed processing                       â”‚
â”‚     â”œâ”€ Schema validation & type conversion                  â”‚
â”‚     â”œâ”€ Data quality checks                                  â”‚
â”‚     â””â”€ Metadata enrichment                                  â”‚
â”‚                                                              â”‚
â”‚                        â†“                                     â”‚
â”‚                                                              â”‚
â”‚  3. LOADING (Storage)                                       â”‚
â”‚     Snowflake Data Warehouse                                â”‚
â”‚     â”œâ”€ RAW Layer: Source of truth (append-only)            â”‚
â”‚     â”œâ”€ STAGING Layer: Temporary processing area            â”‚
â”‚     â”œâ”€ WAREHOUSE Layer: Business-ready (SCD Type 2)        â”‚
â”‚     â””â”€ ANALYTICS Layer: Pre-aggregated views               â”‚
â”‚                                                              â”‚
â”‚                        â†“                                     â”‚
â”‚                                                              â”‚
â”‚  4. VISUALIZATION (Business Intelligence)                   â”‚
â”‚     â”œâ”€ Power BI dashboards                                  â”‚
â”‚     â”œâ”€ Interactive reports & search                         â”‚
â”‚     â”œâ”€ Excel exports                                        â”‚
â”‚     â””â”€ Scheduled data refreshes                             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Snowflake Layer Architecture**

| Layer | Purpose | Update Strategy |
|-------|---------|-----------------|
| **RAW** | Immutable source of truth from API | Append-only |
| **STAGING** | Temporary area for incremental loads | Truncate & reload |
| **WAREHOUSE** | Production tables with SCD Type 2 | Merge (upsert) |
| **ANALYTICS** | Business-ready views | Computed on-demand |

---

## âœ¨ Features

### **ETL Pipeline**
- âœ… Multi-entity support (9+ CEIPAL entities)
- âœ… Distributed processing with PySpark
- âœ… Incremental loads (append/overwrite/merge)
- âœ… Historical tracking (SCD Type 2)
- âœ… Automatic retries & error handling
- âœ… Rate limiting & API throttling
- âœ… Token management & caching
- âœ… Parallel detail enrichment

### **Data Quality**
- âœ… Schema validation on load
- âœ… Type conversion with error handling
- âœ… NULL handling & default values
- âœ… Duplicate detection & deduplication
- âœ… Audit trail & metadata tracking

### **Scalability**
- âœ… Handle millions of records
- âœ… Distributed parallel processing
- âœ… Efficient memory management
- âœ… Optimized Snowflake queries

---

## ğŸ“¦ Prerequisites

### **Software Requirements**

| Component | Version | Purpose |
|-----------|---------|---------|
| **Python** | 3.8+ | ETL scripting |
| **Java** | 8 or 11 | PySpark runtime |
| **Git** | Latest | Version control |

### **Cloud Services**

| Service | Access Level Required |
|---------|----------------------|
| **CEIPAL API** | API key, email, password |
| **Snowflake** | Database creation, warehouse access |
| **Power BI** | Desktop or Pro license |

### **Check Prerequisites**

```bash
# Check Python version
python --version  # Should be 3.8+

# Check Java version
java -version     # Should be 8 or 11

# Check Git
git --version
```

---

## ğŸš€ Quick Start

### **1. Clone Repository**

```bash
cd /path/to/projects
git clone <repository-url>
cd Portals_Integration
```

### **2. Create Virtual Environment**

```bash
# Create virtual environment
python -m venv .venv

# Activate (Windows)
.venv\Scripts\activate

# Activate (macOS/Linux)
source .venv/bin/activate
```

### **3. Install Dependencies**

```bash
pip install -r requirements.txt
```

### **4. Configure Environment**

```bash
# Copy example configuration
cp .env.example .env

# Edit .env with your credentials
# - Add CEIPAL API credentials
# - Add Snowflake connection details
```

### **5. Setup Snowflake**

```bash
# Option 1: Using SnowSQL
snowsql -f sql/snowflake_ddl.sql

# Option 2: Snowflake Web UI
# 1. Open Snowflake UI
# 2. Create new worksheet
# 3. Copy contents of sql/snowflake_ddl.sql
# 4. Execute all statements
```

### **6. Run Your First ETL**

```bash
# Test with small dataset (10 employees)
python scripts/dump_ceipal_employees_spark.py \
  --employees \
  --limit 10 \
  --max-pages 1

# Verify in Snowflake
# SELECT COUNT(*) FROM CEIPAL_DW.RAW.EMPLOYEES;
```

ğŸ“– **For detailed setup instructions, see [docs/QUICKSTART.md](docs/QUICKSTART.md)**

---

## ğŸ“ Project Structure

```
Portals_Integration/
â”œâ”€â”€ .git/                          # Git version control
â”œâ”€â”€ .venv/                         # Python virtual environment
â”œâ”€â”€ docs/                          # Documentation
â”‚   â””â”€â”€ QUICKSTART.md              # Quick setup guide
â”œâ”€â”€ output/                        # ETL outputs (gitignored)
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ scripts/                       # ETL scripts
â”‚   â”œâ”€â”€ ceipal_sync.py             # Token manager & API client
â”‚   â””â”€â”€ dump_ceipal_employees_spark.py  # Main ETL script
â”œâ”€â”€ sql/                           # Database scripts
â”‚   â””â”€â”€ snowflake_ddl.sql          # Snowflake schema setup
â”œâ”€â”€ .env                           # Configuration (secret, not in git)
â”œâ”€â”€ .env.example                   # Configuration template
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”œâ”€â”€ README.md                      # This file
â””â”€â”€ requirements.txt               # Python dependencies
```

---

## âš™ï¸ Configuration

### **Environment Variables**

Create a `.env` file based on `.env.example`:

```bash
# ============== CEIPAL API ==============
CEIPAL_BASE_URL=https://api.ceipal.com
CEIPAL_EMAIL=your-email@company.com
CEIPAL_PASSWORD=your-password
CEIPAL_API_KEY=your-api-key

# ============== Snowflake ==============
SNOWFLAKE_ACCOUNT=your-account.us-east-1
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ROLE=SYSADMIN
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DATABASE=CEIPAL_DW
SNOWFLAKE_SCHEMA=RAW

# ============== ETL Settings ==============
LOAD_STRATEGY=append              # append, overwrite, or merge
DETAIL_RPS=2.0                    # API requests per second
DETAIL_WORKERS=4                  # Parallel workers

# ============== PySpark ==============
SPARK_DRIVER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=4g
```

### **Load Strategies**

| Strategy | Use Case | Speed | Duplicates |
|----------|----------|-------|------------|
| **append** | Initial load, logs | Fast | Possible |
| **overwrite** | Full refresh | Medium | No |
| **merge** | Incremental updates | Slower | No |

---

## ğŸ’» Usage

### **Basic Commands**

```bash
# Load employees
python scripts/dump_ceipal_employees_spark.py --employees

# Load employees with full details
python scripts/dump_ceipal_employees_spark.py --employees --with-details

# Load specific entity
python scripts/dump_ceipal_employees_spark.py --projects --with-details

# Load all entities
python scripts/dump_ceipal_employees_spark.py --all --with-details
```

### **Advanced Options**

```bash
# Incremental load with merge strategy
python scripts/dump_ceipal_employees_spark.py \
  --employees \
  --load-strategy merge \
  --with-details

# Custom rate limiting (8 parallel workers, 5 req/sec)
python scripts/dump_ceipal_employees_spark.py \
  --employees \
  --with-details \
  --workers 8 \
  --rps 5.0

# Limit records for testing
python scripts/dump_ceipal_employees_spark.py \
  --employees \
  --limit 50 \
  --max-pages 2

# Debug mode
python scripts/dump_ceipal_employees_spark.py \
  --employees \
  --debug
```

### **Command-Line Options**

| Option | Description | Default |
|--------|-------------|---------|
| `--employees` | Load employees | Default |
| `--projects` | Load projects | - |
| `--clients` | Load clients | - |
| `--placements` | Load placements | - |
| `--all` | Load all entities | - |
| `--with-details` | Fetch detailed info per record | False |
| `--limit` | Records per page | 100 |
| `--max-pages` | Maximum pages to fetch | None |
| `--workers` | Parallel workers | 4 |
| `--rps` | Rate limit (requests/sec) | 2.0 |
| `--load-strategy` | append/overwrite/merge | append |
| `--debug` | Enable debug logging | False |

---

## ğŸ”„ Data Pipeline

### **1. Extraction Phase**

```python
# The script automatically:
1. Authenticates with CEIPAL API
2. Fetches paginated data (limit=100 per page)
3. Enriches records with detail API calls (if --with-details)
4. Handles rate limiting and retries
5. Caches authentication tokens
```

### **2. Transformation Phase**

```python
# PySpark transformations:
1. Convert Python lists to Spark DataFrames
2. Apply schema validation
3. Add metadata columns (load_timestamp, source_system)
4. Type conversions and NULL handling
5. Partition data for optimal loading
```

### **3. Loading Phase**

```python
# Snowflake loading:
1. Write to STAGING schema (if merge strategy)
2. Load to RAW layer with append/overwrite/merge
3. Update WAREHOUSE layer (SCD Type 2)
4. Refresh ANALYTICS views
```

### **Verify Data in Snowflake**

```sql
-- Check record counts
SELECT COUNT(*) FROM CEIPAL_DW.RAW.EMPLOYEES;

-- View recent records
SELECT * FROM CEIPAL_DW.RAW.EMPLOYEES
ORDER BY load_timestamp DESC
LIMIT 10;

-- Check data quality
SELECT
    COUNT(*) AS total_records,
    SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) AS missing_email,
    SUM(CASE WHEN department IS NULL THEN 1 ELSE 0 END) AS missing_dept
FROM CEIPAL_DW.RAW.EMPLOYEES
WHERE load_date = CURRENT_DATE();
```

---

## ğŸ“Š Power BI Integration

### **Setup Power BI Connection**

1. **Open Power BI Desktop**

2. **Get Data â†’ Snowflake**
   ```
   Server: your-account.snowflakecomputing.com
   Warehouse: COMPUTE_WH
   Database: CEIPAL_DW
   ```

3. **Select Tables**
   ```
   WAREHOUSE.DIM_EMPLOYEES
   WAREHOUSE.DIM_CLIENTS
   WAREHOUSE.FACT_PLACEMENTS
   ANALYTICS.V_REVENUE_BY_CLIENT
   ```

4. **Build Reports**
   - Drag fields to visuals
   - Create filters and slicers
   - Add search boxes
   - Enable Excel export

### **Sample Power BI Queries**

```powerquery
// Active Employees
let
    Source = Snowflake.Databases("your-account.snowflakecomputing.com"),
    Database = Source{[Name="CEIPAL_DW"]}[Data],
    Schema = Database{[Name="WAREHOUSE"]}[Data],
    Table = Schema{[Name="DIM_EMPLOYEES"]}[Data],
    Filtered = Table.SelectRows(Table, each [is_current] = true)
in
    Filtered
```

### **Export to Excel**

Users can:
- Click "Export data" on any visual
- Export entire table to Excel
- Create custom exports with filters

---

## ğŸ› Troubleshooting

### **Java Not Found**

```bash
# Error: JAVA_HOME not set

# Solution (Windows)
set JAVA_HOME=C:\Program Files\Java\jdk-11

# Solution (macOS)
export JAVA_HOME=$(/usr/libexec/java_home)

# Solution (Linux)
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk
```

### **Snowflake Connection Timeout**

```bash
# Check account format
# Correct: account-name.region.cloud
# Example: xyz12345.us-east-1.aws

# Test connection
python -c "
from snowflake.connector import connect
conn = connect(
    account='your-account',
    user='your-user',
    password='your-pass'
)
print('Connected!')
conn.close()
"
```

### **CEIPAL API Rate Limit**

```bash
# Error: 429 Too Many Requests

# Solution: Reduce rate
python scripts/dump_ceipal_employees_spark.py \
  --employees \
  --rps 0.5 \
  --workers 2
```

### **PySpark Out of Memory**

```bash
# Increase driver memory
export SPARK_DRIVER_MEMORY=8g

# Or in .env file
SPARK_DRIVER_MEMORY=8g
```

### **Common Issues**

| Issue | Solution |
|-------|----------|
| Import error for PySpark | `pip install -r requirements.txt` |
| Snowflake authentication failed | Check credentials in `.env` |
| No data in Snowflake | Verify ETL script completed successfully |
| Duplicate records | Use `--load-strategy merge` |

---

## ğŸ“– Documentation

- **[Quick Start Guide](docs/QUICKSTART.md)** - Get running in 10 minutes
- **[Snowflake Setup](sql/snowflake_ddl.sql)** - Database schema documentation
- **[API Documentation](https://api.ceipal.com/docs)** - CEIPAL API reference

---

## ğŸ¤ Contributing

### **Development Setup**

```bash
# Install dev dependencies
pip install -r requirements.txt

# Run linting
pylint scripts/*.py

# Run tests (if available)
pytest tests/
```

### **Code Style**

- Follow PEP 8 guidelines
- Use type hints where possible
- Add docstrings to functions
- Keep functions focused and testable

---

## ğŸ“ License

Internal use only. Confidential and proprietary.

---

## ğŸ“ Support

For issues or questions:
- Create an issue in this repository
- Contact the Data Engineering team
- Email: data-team@company.com

---

## ğŸ”„ Changelog

### v2.0.0 (2025-10-10)
- âœ… Removed legacy file-based scripts
- âœ… Reorganized project structure
- âœ… Added professional documentation
- âœ… Initialized git repository
- âœ… Enhanced .gitignore for production use
- âœ… Renamed requirements-spark.txt to requirements.txt

### v1.0.0 (2024-XX-XX)
- Initial PySpark + Snowflake implementation
- Multi-entity support
- SCD Type 2 for dimensions
- Analytics views

---

**Built with â¤ï¸ for Enterprise Data Analytics**
